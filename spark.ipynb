{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243e44c8-4712-4d1e-8e27-0fb169168c79",
   "metadata": {},
   "source": [
    "## Run docker dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68fd85-fcda-44f2-b62b-329c17621d1c",
   "metadata": {},
   "source": [
    "## Make sure spark notebook is present in docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d703aa-78e4-4486-9b44-022a4bd53a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run -it -p 8888:8888 -p 4040:4040 --name my_jupyter -v \"/:/root\" jupyter/pyspark-notebook\n",
    "\n",
    "# # For coming back to saved work, RUN\n",
    "# docker start -ai my_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f61aa-772c-45ab-8efe-960630d14598",
   "metadata": {},
   "source": [
    "## make sure docker is running with mongo and postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0d74c1-21d9-44ec-9d8f-6219613dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure mongodb and postgres are running in docker for the code below to work\n",
    "# docker run -d --name mongodb -p 27017:27017 mongo\n",
    "# docker run -d --name postgres -p 5432:5432 -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -e POSTGRES_DB=retail_db postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4c07d6-2758-4d13-81ff-3a20eabe984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Jupyter with persistent storage, mount everything at root\n",
    "## THIS NEEDS TO BE TEST\n",
    "# docker run -it --rm -p 8888:8888 -p 4040:4040  \\\n",
    "#   -v \"/:/root\" \\\n",
    "#   jupyter/pyspark-notebook\n",
    "\n",
    "\n",
    "# # # Start MongoDB & PostgreSQL with persistent storage:\n",
    "# docker run -d --name mongodb -p 27017:27017 \\\n",
    "#   -v \"/mongo_data:/data/db\" mongo\n",
    "\n",
    "# docker run -d --name postgres -p 5432:5432 \\\n",
    "#   -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -e POSTGRES_DB=retail_db \\\n",
    "#   -v \"/pg_data:/var/lib/postgresql/data\" postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc8734-2c45-4203-92d5-f8495db13424",
   "metadata": {},
   "source": [
    "## install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce651b84-c0d3-4ba6-bd02-883b3bb7a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (22 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m219.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m153.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m271.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m312.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, psycopg2-binary, dnspython, pymongo\n",
      "Successfully installed dnspython-2.7.0 psycopg2-binary-2.9.10 py4j-0.10.9.7 pymongo-4.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyspark pymongo psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf2ca1-16f5-41f7-a457-85cf81fe8704",
   "metadata": {},
   "source": [
    "## install postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff37e80-2d40-438a-ae44-f8087123de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-14 22:11:42--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1046274 (1022K) [application/java-archive]\n",
      "Saving to: ‘./postgresql-42.5.0.jar’\n",
      "\n",
      "postgresql-42.5.0.j 100%[===================>]   1022K  33.1KB/s    in 25s     \n",
      "\n",
      "2025-02-14 22:12:07 (41.5 KB/s) - ‘./postgresql-42.5.0.jar’ saved [1046274/1046274]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# click File-> New -> Terminal\n",
    "# Run the following in the terminal inside this environment\n",
    "# wget -P ./ https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
    "\n",
    "# # TRY THIS NEXT RUN\n",
    "# !curl -o ./postgresql-42.5.0.jar https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
    "#  OR\n",
    "!wget -P ./ https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8082d67-1f70-42c6-9fb2-2674ea63011d",
   "metadata": {},
   "source": [
    "# MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc0fb22-4e09-48f1-ac9b-4350059e58ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Columns in dataset: Index(['Campaign_ID', 'Company', 'Campaign_Type', 'Target_Audience',\n",
      "       'Duration', 'Channel_Used', 'Conversion_Rate', 'Acquisition_Cost',\n",
      "       'ROI', 'Location', 'Language', 'Clicks', 'Impressions',\n",
      "       'Engagement_Score', 'Customer_Segment', 'Date'],\n",
      "      dtype='object')\n",
      "Old data cleared from MongoDB.\n",
      "Data successfully inserted into MongoDB!\n",
      "Data successfully transferred to PostgreSQL!\n",
      "+-----------+---------+\n",
      "|customer_id|purchases|\n",
      "+-----------+---------+\n",
      "|         12|        5|\n",
      "|          1|        5|\n",
      "|         13|        5|\n",
      "|          6|        5|\n",
      "|          3|        5|\n",
      "|          5|        5|\n",
      "|         15|        5|\n",
      "|          9|        5|\n",
      "|          4|        5|\n",
      "|          8|        5|\n",
      "|          7|        5|\n",
      "|         10|        5|\n",
      "|         11|        5|\n",
      "|         14|        5|\n",
      "|          2|        5|\n",
      "+-----------+---------+\n",
      "\n",
      "+-----------+--------------------+\n",
      "|customer_id|         total_spent|\n",
      "+-----------+--------------------+\n",
      "|         12|0.000000000000000000|\n",
      "|          1|0.000000000000000000|\n",
      "|         13|0.000000000000000000|\n",
      "|          6|0.000000000000000000|\n",
      "|          3|0.000000000000000000|\n",
      "+-----------+--------------------+\n",
      "\n",
      "Data exported to Parquet!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session inside Docker with PostgreSQL JDBC driver\n",
    "# Initialize Spark with proper JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailAnalysis\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.5.0.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.5.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Version:\", spark.version)\n",
    "\n",
    "# Load data locally\n",
    "df = pd.read_csv(\"marketing_campaign_dataset_head.csv\").head(15) # make sure you have this file at the root\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# 1. Connect to MongoDB (Ensure MongoDB is running in a container)\n",
    "MONGO_URI = \"mongodb://host.docker.internal:27017/\"  # Use this inside Docker\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"retail_db\"]\n",
    "collection = db[\"marketing_campaign\"]\n",
    "\n",
    "# Clear existing data to avoid duplicates\n",
    "collection.delete_many({})\n",
    "print(\"Old data cleared from MongoDB.\")\n",
    "\n",
    "# Convert DataFrame to dictionary records and insert into MongoDB\n",
    "collection.insert_many(df.to_dict(orient=\"records\"))\n",
    "print(\"Data successfully inserted into MongoDB!\")\n",
    "\n",
    "# 2. Define PostgreSQL Schema\n",
    "POSTGRES_URI = \"jdbc:postgresql://host.docker.internal:5432/retail_db\"\n",
    "postgres_conn = psycopg2.connect(\n",
    "    dbname=\"retail_db\",\n",
    "    user=\"admin\",\n",
    "    password=\"password\",\n",
    "    host=\"host.docker.internal\",\n",
    "    port=5432  # Ensure correct port\n",
    ")\n",
    "cursor = postgres_conn.cursor()\n",
    "\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS marketing_campaign (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        customer_id INT,\n",
    "        amount_spent DECIMAL DEFAULT 0,\n",
    "        rating DECIMAL DEFAULT 0,\n",
    "        purchase_date DATE\n",
    "    );\n",
    "    \"\"\"\n",
    ")\n",
    "postgres_conn.commit()\n",
    "\n",
    "# 3. Transfer Data from MongoDB to PostgreSQL\n",
    "records = collection.find({}, {\"_id\": 0})\n",
    "for record in records:\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO marketing_campaign (customer_id, amount_spent, rating, purchase_date)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (\n",
    "            record.get(\"Campaign_ID\", None),  # Adjusted based on dataset\n",
    "            record.get(\"Amount\", 0),  # Default to 0 if missing\n",
    "            record.get(\"Rating_Score\", 0),  # Default to 0 if missing\n",
    "            record.get(\"Date\", None)  # Keep null if missing\n",
    "        ),\n",
    "    )\n",
    "\n",
    "postgres_conn.commit()\n",
    "cursor.close()\n",
    "postgres_conn.close()\n",
    "print(\"Data successfully transferred to PostgreSQL!\")\n",
    "\n",
    "# 4. Spark SQL Analysis\n",
    "df_spark = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", POSTGRES_URI)\n",
    "    .option(\"dbtable\", \"marketing_campaign\")\n",
    "    .option(\"user\", \"admin\")\n",
    "    .option(\"password\", \"password\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_spark.createOrReplaceTempView(\"marketing_campaign\")\n",
    "\n",
    "# Example queries\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT customer_id, COUNT(*) AS purchases\n",
    "    FROM marketing_campaign\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY purchases DESC\n",
    "    \"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT customer_id, SUM(amount_spent) AS total_spent\n",
    "    FROM marketing_campaign\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    ").show()\n",
    "\n",
    "# Convert purchase_date to proper Date format if necessary\n",
    "df_spark = df_spark.withColumn(\"purchase_date\", df_spark[\"purchase_date\"].cast(\"date\"))\n",
    "\n",
    "# Export to Parquet\n",
    "df_spark.write.mode(\"overwrite\").parquet(\"marketing_campaign.parquet\")\n",
    "print(\"Data exported to Parquet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6fc14-1c77-46f5-a47c-ee78065f9bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
